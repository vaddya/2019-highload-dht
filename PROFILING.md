# Profiling

1. [Single node profiling](#1-single-node)
2. [Thread-safe single node](#2-thread-safe-single-node)
3. [Asynchronous server](#3-asynchronous-server)
4. [Sharding](#4-sharding)
5. [Replication](#5-replication)
6. [Futures](#6-futures)

## 1. Single node

### `GET /v0/entity`

* При запросе по несуществующему ключу, значительная часть времени 
и ресурсов аллокации тратится на заполнение стектрейса для 
исключения `NoSuchElementException`, генерируемого в методе 
`DAO::get`. Решение -- генерировать кастомное исключение 
`NoSuchEntityException` без заполнения стектрейса. Было 15.89% 
от всего времени выполнения, стало 0.41%; 99.9-й персентиль 
времени ответа уменьшился с 7.06 мс до 2.73 мс.

* При запросе по существующему ключу, время в основном тратится 
на метод `SSTable::keyAt`, в котором выполняется поиск нужного 
ключа в файле на диске. Данная операция использует бинарный поиск 
по отсортированному массиву пар ключ-значение, поэтому для 
увеличения скорости поиска нужно хранить дополнительную 
информацию о SSTable. Например, можно использовать Bloom 
filter, чтобы снизить шанс поиска отсутствующего ключа, или
хранить дополнительную информацию о блоках ключей в файле,
позволяя быстро найти блок, в котором может находится искомый ключ.

* Большую часть аллокаций занимает создание объектов
`java.nio.DirectByteBufferR` в том же методе `SSTable::keyAt`.
Методы `ByteBuffer::duplicate` и `ByteBuffer::slice` используются 
для создания ключей, чтобы затем сравнить их друг с другом в 
бинарном поиске.

* Примерно одинаковое время занимает поиск по ключу в `DAO` и 
запись HTTP-ответа в сокет. При записи происходит вызов нативного 
блокирующего метода `write`. Таким образом, один поток и производит 
поиск по базе данных, и занимается обработкой HTTP-запроса (чтение
параметров запроса, запись ответа и т.д.). Разумным решением было 
бы использовать для этих задач различные потоки (пулы потоков),
что позволит одному потоку обрабатывать большее число запросов. 

### `PUT /v0/entity`

* Больше половины времени выполнения `DAO::upsert` тратится на
периодический сброс MemTable на диск, тем самым замедляя такие
запросы на несколько порядков. Сброс на диск предполагает работу
с вводом-выводом, включая блокирующий вызов `write`. Из-за этого 
90-й перцентиль (при 600 тыс. запросов на вставку через `wrk`) 
оказался равен 4 мс, в то время как 99-й перцентиль составил более 
1 с (увеличение на 3 порядка!). Логичным решением будет разделение 
задач вставки новой записи в MemTable и сброса MemTable на диск.
Для этого можно при достижении предельного размера MemTable 
переключаться на новый экземпляр MemTable, при этом асинхронно 
отправив заполненную таблицу на сохранение.

* При обычной вставки в MemTable все время тратится на поиск 
позиции для вставки в `java.util.TreeMap`.

* Значительная часть аллокаций происходит при работе с `ByteBuffer` 
во время сброса MemTable на диск. Дополнительные аллокации 
происходят для создания read-only буферов, которые будут храниться
в MemTable. В данном случае, от их создания можно отказаться,
если потребовать, чтобы передаваемые в `DAO::upsert` буферы,
содержащие ключ и значение, были неизменны. Однако, это потребует 
изменения существующего API `DAO`.

* Аналогично GET-запросам, больше половины времени тратится на 
запись ответа в сокет.

### `DELETE /v0/entity`

* В LSM-based хранилищах операции вставки и удаления практически
не отличаются (удаление -- вставка ключа со специальной пометкой
вместо значения), поэтому все сказанное про `PUT`, применимо и 
к `DELETE`.

## 2. Thread-safe single node

### `GET /v0/entity`

* Большую часть времени занимает работа с итераторами в методах 
`IteratorUtils::collectIterators` и `IteratorUtils::mergeIterators`.

* На данный момент не реализован фоновый compaction-поток, из-за 
чего сложность и время поиска по ключу увеличивается с увеличением
числа таблиц на диске.

* Большую часть аллокаций по-прежнему занимает создание объектов 
`java.nio.DirectByteBufferR`.

* При профилировании на блокировки, не была захвачена ни одна
трасса выполнения. Это связано с использованием `ReadWriteLock`,
который позволяет нескольким читателям не блокироваться и 
выполнять поиск параллельно.

### `PUT/DELETE /v0/entity`

* Из flame graph видно, что сбросом MemTable на диск теперь
занимаются выделенные потоки в Executor, что позволяет уменьшить 
99-й перцентиль до 3 мс (вместо 1 с на предыдущем этапе).

* Все еще значительную часть времени (3/4) от обработки запроса 
занимает запись ответа в сокет. 

* Время обработки вставки складывается из создания/копирования
буферов и вставки в потокобезопасную `ConcurrentSkipListMap`.

* При вставке аллокации происходят только при создании `ByteBuffer`
и `TableEntry` для хранения значений в MemTable. В flush-потоках 
аллоцируются `ByteBuffer`, в которые в определенном формате 
укладывается MemTable и сбрасывается на диск.

* При профилировании на lock был замечен только метод 
`TimeUtils::currentTimeNanos`, в котором используется синхронизация
для эмуляции определения времени с точностью до наносекунд.
Очевидно, что в программе используются и другие блокировки, но
т.к. даже при уменьшении интервала профилирования до 100 мкс
других путей не было обнаружено, значит потоки не держат
блокировки по долгу: под блокировкой делается только минимальная 
необходимая работа.

## 3. Asynchronous server

* За счет разделения потоков на пулы селекторов и воркеров удалось 
разбить работу с сокетом (открытие, чтение, запись) и операции 
над хранилищем. Это позволяет независимо масштабировать ресурсы 
(потоки) для этих задач и уменьшить время работы методов сервера,
которые не используют хранилище. 

* При смешанной нагрузке видно, что запрос к `/v0/status` 
отрабатывает примерно в два раза быстрее, чем обращения к 
хранилищу, таким образом долгие обращения к базе не тормозят
другие запросы.

* Запросы на чтение работают на 30-40% дольше, чем запросы на 
вставку (не столь значительная разница может объясняться 
использованием SSD).

### `GET /v0/entity`

* Селекторы заняты только опросом сокетов, разбором запроса и
постановкой задач в очередь. Воркеры заняты получением задачи 
из этой очереди, поиском в хранилище (большая часть работы) и 
записью ответа в сокет. Совсем избавить воркеров от работы с 
сокетом не удалось.

* Большую часть аллокаций производят воркеры при поиске по ключу
(создание буферов с ключами для сравнения) и селекторы при 
разборе запроса.

* Селекторы и воркеры используют синхронизированные методы
класса `Session` для чтения/записи ответа из/в сокет.

### `PUT/DELETE /v0/entity`

* На запись ответа воркер тратит больше времени, чем на вставку
в MemTable.

* Большую часть аллокаций производят селекторы во время разбора
запроса.

* Воркеры активно используют блокировку для синхронизации доступа
к MemTable и определения времени в `TimeUtils::currentTimeNanos`.

### `GET /v0/entities`

* При профилировании используются запросы вида 
`/v0/entities?start=keyN&end=keyM`, где N in 1..8, M = N + 1. При таких
запросах сервер отдавал клиенту около 10К пар ключ-значение.

* Сервер потянул только 100 RPS из-за большого тела ответа
(5410 requests in 1.00m, 0.91GB read).

* Не используем воркеров, потому что создание итератора это
быстрая операция относительно записи ответа в сокет, и большее
количество ресурсов было бы потрачено на синхронизацию селекторов
и воркеров.

* Примерно 1/8 времени обработки запроса тратится на запрос
следующего значения у итератора. Можно попробовать заранее
(спекулятивно) запрашивать следующие значения.

* Происходит большое количество аллокаций памяти для получения
следующего элемента итератора, однако, можно предположить, что
такие объекты не покинут Эдем и будут быстро собраны GC. 

* Блокировки на флейм граф не попали, потому что используются
`ReadWriteLock`, позволяющие работать нескольким читателям 
параллельно. 

## 4. Sharding

### `GET/PUT/DELETE /v0/entity`

* При профилировании ноды, на которую идут все запросы, видно,
что теперь часть времени тратится на проксирования запроса
к соседней ноде: отправка запроса, получение ответа, запись ответа
(два дополнительных стека вызовов TCP по сравнению с single-node).
При этом проксирование происходит в воркерах, что позволяет 
оставить в селекторах только разбор запроса и поставноку задач
в очередь воркерам.

* Дополнительный overhead добавило определение primary нод для
ключей: хеширование ключа, поиск нужной ноды в кольце (Consistent
Hashing).

* Нода, к которой только проксировались запросы, не пыталась
проксировать запрос другой ноде, значит определение ноды по ключу 
на всех нодах было одинаковым и работает верно.

* При смешанной нагрузке (`PUT` на одну из нод кластера, `GET` на 
другую) 99-ые перцентили оказались примерно одинаковые (6.43 мс и 
6.64 мс соответственно). Без SSD получение по ключу, скорее всего,
работало бы значительно дольше (из-за обращения к HDD диску).

* 99.9-ый перцентиль `PUT`-запросов, по сравнению с single-node версией, 
увеличился с 2.61 мс до 5.77 мс. 99.9-ый перцентиль `GET`-запросов 
увеличился с 1.78 мс до 4.05 мс. Однако достигнута главная цель -- 
общая емкость кластера растет пропорционально числу нод в кластере.
Следовательно, наше хранилище горизонтально масштабируется.

## 5. Replication

### `GET/PUT/DELETE /v0/entity`

* Большая часть CPU тратится на передачу задач между слекторами
и воркерами. Проблема кроется в том, что воркер при сборе ответов
от БД и других нод блокируется в ожидании ответов. Увеличение числа
воркеров может улучшить эту ситуацию, но при определенной ситуации 
все воркеры могут снова оказаться заблокированными в ожидании ответов.
Для решения этой проблемы необходимо использовать асинхронный клиент
для общения с другими нодами.

* Нода, получающая запросы тратит время на планирование задач и сбор 
ответов, в то время как нода, к которой проксируются запросы, тратит 
время только на общение с локальной БД.

* Треды блокируются при работе с сессией. В теории, при использовании
асинхронного клиента, блокировки должны остаться только для синхронизации
доступа к БД.

* Благодаря использованию `Future`, запросы к другим нодам выполняются 
параллельно, за счет чего время ответа ~ времени ответа самой медленной
ноды. Однако данный механизм не позволяет дождаться, например, двух
самых быстрых ответов при `replicas=2/3`.

## 6. Futures

### `GET/PUT/DELETE /v0/entity`

* Концептуально теперь используется 4 пула потоков (видно в VisualVM):
1. Селекторы one-nio занимаются разбором запросов и шедулингом запросов
к локальному хранилищу и к удаленным нодам через асинхронные HTTP-клиенты.
2. Воркеры one-nio обрабатывают только запросы к локальному хранилищу.
3. Потоки асинхронных HTTP клиентов. У каждого клиента есть SelectorManager,
который по необходимости создает дополнительных воркеров (причем может
создавать их десятками) для обработки ответа от другой ноды.
4. Несколько флашеров занимаются сбросом закрытым MemTable на диск.

* Найти наш код на флейм графах теперь нелегко.

* Много времени тратится на синхронизацию между потоками через примитивы
синхронизации и SynchronousQueue. Селекторы активно шедулят запросы, а 
воркеры занимаются чтением/записью из/в сокеты.

* Очень много аллокаций связано с обработкой фьюч. Селекторы one-nio 
также активно создают фьючи при шедулинги запросов к локальному хранилищу
и к удаленным нодам. При `GET` запросах хранилище создает много `ByteBuffer`
при поиске ключей в SSTable.

* По сравнению с предыдущим этапам, количество пойманных `async-profiler` 
семплов с локами выросло на несколько поярдков. На флейм графе локов видно, 
что блокировки в основном связаны с двумя вещами. Во-первых, фьючи 
являются видом межпотокового взаимодействия, поэтому внутри используются 
примитивы синхронизации. Во-вторых, асинхронные HTTP-клиенты используют 
блокировки для синхронизации доступа к сокету, к пулу соединений, при 
чтении/записи ответа/запроса и т.д.

* Throughput `PUT` запросов увеличился в 3 раза по сравнению с предыдущим
этапом (с 30327 requests in 30.02s до 91636 requests in 30.01s), однако
время обработки увеличилось для 99.9-го перцентиля (с 5.65ms до 11.95ms).
Снижение времени, скорее всего, вызвано межпотоковым взаимодействием
и увеличившимся числом блокировок в программе. Пропускная способность 
`GET` запросов не изменилась, что скорее всего вызвано явлением 
read-amplification, свойственным LSM-based хранилищам.
