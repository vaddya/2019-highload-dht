# Single node profiling

## `GET`

* При запросе по несуществующему ключу, значительная часть времени 
и ресурсов аллокации тратится на заполнение стектрейса для 
исключения `NoSuchElementException`, генерируемого в методе 
`DAO::get`. Решение -- генерировать кастомное исключение 
`NoSuchEntityException` без заполнения стектрейса. Было 15.89% 
от всего времени выполнения, стало 0.41%; 99.9-й персентиль 
времени ответа уменьшился с 7.06 мс до 2.73 мс.

* При запросе по существующему ключу, время в основном тратится 
на метод `SSTable::keyAt`, в котором выполняется поиск нужного 
ключа в файле на диске. Данная операция использует бинарный поиск 
по отсортированному массиву пар ключ-значение, поэтому для 
увеличения скорости поиска нужно хранить дополнительную 
информацию о SSTable. Например, можно использовать Bloom 
filter, чтобы снизить шанс поиска отсутствующего ключа, или
хранить дополнительную информацию о блоках ключей в файле,
позволяя быстро найти блок, в котором может находится искомый ключ.

* Большую часть аллокаций занимает создание объектов
`java.nio.DirectByteBufferR` в том же методе `SSTable::keyAt`.
Методы `ByteBuffer::duplicate` и `ByteBuffer::slice` используются 
для создания ключей, чтобы затем сравнить их друг с другом в 
бинарном поиске.

* Примерно одинаковое время занимает поиск по ключу в `DAO` и 
запись HTTP-ответа в сокет. При записи происходит вызов нативного 
блокирующего метода `write`. Таким образом, один поток и производит 
поиск по базе данных, и занимается обработкой HTTP-запроса (чтение
параметров запроса, запись ответа и т.д.). Разумным решением было 
бы использовать для этих задач различные потоки (пулы потоков),
что позволит одному потоку обрабатывать большее число запросов. 

## `PUT`

* Больше половины времени выполнения `DAO::upsert` тратится на
периодический сброс MemTable на диск, тем самым замедляя такие
запросы на несколько порядков. Сброс на диск предполагает работу
с вводом-выводом, включая блокирующий вызов `write`. Из-за этого 
90-й перцентиль (при 600 тыс. запросов на вставку через `wrk`) 
оказался равен 4 мс, в то время как 99-й перцентиль составил более 
1 с (увеличение на 3 порядка!). Логичным решением будет разделение 
задач вставки новой записи в MemTable и сброса MemTable на диск.
Для этого можно при достижении предельного размера MemTable 
переключаться на новый экземпляр MemTable, при этом асинхронно 
отправив заполненную таблицу на сохранение.

* При обычной вставки в MemTable все время тратится на поиск 
позиции для вставки в `java.util.TreeMap`.

* Значительная часть аллокаций происходит при работе с `ByteBuffer` 
во время сброса MemTable на диск. Дополнительные аллокации 
происходят для создания read-only буферов, которые будут храниться
в MemTable. В данном случае, от их создания можно отказаться,
если потребовать, чтобы передаваемые в `DAO::upsert` буферы,
содержащие ключ и значение, были неизменны. Однако, это потребует 
изменения существующего API `DAO`.

* Аналогично GET-запросам, больше половины времени тратится на 
запись ответа в сокет.

## `DELETE`

* В LSM-based хранилищах операции вставки и удаления практически
не отличаются (удаление -- вставка ключа со специальной пометкой
вместо значения), поэтому все сказанное про `PUT`, применимо и 
к `DELETE`.

# Thread-safe single node profiling

## `GET`

* Большую часть времени занимает работа с итераторами в методах 
`IteratorUtils::collectIterators` и `IteratorUtils::mergeIterators`.

* На данный момент не реализован фоновый compaction-поток, из-за 
чего сложность и время поиска по ключу увеличивается с увеличением
числа таблиц на диске.

* Большую часть аллокаций по-прежнему занимает создание объектов 
`java.nio.DirectByteBufferR`.

* При профилировании на блокировки, не была захвачена ни одна
трасса выполнения. Это связано с использованием `ReadWriteLock`,
который позволяет нескольким читателям не блокироваться и 
выполнять поиск параллельно.

## `PUT`/`DELETE`

* Из flame graph видно, что сбросом MemTable на диск теперь
занимаются выделенные потоки в Executor, что позволяет уменьшить 
99-й перцентиль до 3 мс (вместо 1 с на предыдущем этапе).

* Все еще значительную часть времени (3/4) от обработки запроса 
занимает запись ответа в сокет. 

* Время обработки вставки складывается из создания/копирования
буферов и вставки в потокобезопасную `ConcurrentSkipListMap`.

* При вставке аллокации происходят только при создании `ByteBuffer`
и `TableEntry` для хранения значений в MemTable. В flush-потоках 
аллоцируются `ByteBuffer`, в которые в определенном формате 
укладывается MemTable и сбрасывается на диск.

* При профилировании на lock был замечен только метод 
`TimeUtils::currentTimeNanos`, в котором используется синхронизация
для эмуляции определения времени с точностью до наносекунд.
Очевидно, что в программе используются и другие блокировки, но
т.к. даже при уменьшении интервала профилирования до 100 мкс
других путей не было обнаружено, значит потоки не держат
блокировки по долгу: под блокировкой делается только минимальная 
необходимая работа.
